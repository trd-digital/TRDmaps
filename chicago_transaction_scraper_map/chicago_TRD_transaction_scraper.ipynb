{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6c89b50",
   "metadata": {},
   "source": [
    "## TO DO:\n",
    "\n",
    "1. Modify the parser to pull the entire story text instead of just the parts tagged as `<p>`, so it'll pull Miranda's stuff as well.\n",
    "2. Run scraper for first 8,000 URLs, since it keeps getting hung up for whatever reason.\n",
    "3. Put the 8,000 results into a DF and then use Python magic to figure out which URLs need scraped, then get the rest of them.\n",
    "4. Pester A3 for all of 2020's URLs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a811c0dc",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4c1dd1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386568bb",
   "metadata": {},
   "source": [
    "## Set Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5bb7d93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 10)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_colwidth', 50) ### Default 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6c2ffd",
   "metadata": {},
   "source": [
    "## Data Read-in"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e72640",
   "metadata": {},
   "source": [
    "<em>All</em> Chicago URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c8eaced7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Chicago URLS by year - 2017.csv',header=None)\n",
    "df2 = pd.read_csv('Chicago URLS by year - 2018.csv',header=None)\n",
    "df3 = pd.read_csv('Chicago URLS by year - 2019.csv',header=None)\n",
    "df4 = pd.read_csv('Chicago URLS by year - 2020.csv',header=None)\n",
    "df5 = pd.read_csv('Chicago URLS by year - 2021.csv',header=None)\n",
    "df6 = pd.read_csv('Chicago URLS by year - 2022.csv',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6e6f0fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [df,df2,df3,df4,df5,df6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "abd59738",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9698d5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "359b3835",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={0:'URL'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "98f965b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12222"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e836203c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.iloc[:7000,:]\n",
    "df2 = df.iloc[7000:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c040ad00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7000\n",
      "------\n",
      "5222\n"
     ]
    }
   ],
   "source": [
    "print(len(df1))\n",
    "print('------')\n",
    "print(len(df2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4b49a37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Original Data set - kept for backup\n",
    "# df = pd.read_csv('TRD_CHI-Coverage-Valid-2022-12-01 - Table.csv')\n",
    "# df = df.drop(columns='Last crawled')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edfdf697",
   "metadata": {},
   "source": [
    "## Test Ground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d8eb6c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# page = requests.get('https://therealdeal.com/chicago/2022/09/22/hinsdale-mckenna-keep-top-spot-among-dupage-countys-priciest-home-sales/')\n",
    "# soup = BeautifulSoup(page.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b73bb1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# soup.find('h1').text.strip() ### Title\n",
    "# soup.find('h2').text.strip() ### Subhead\n",
    "# soup.find('div', class_='col-7 col-md-3 order-md-3 text-right').text.strip() ### Pub_Date\n",
    "# soup.find_all('p')[2].text.strip() ### Lede"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cd6734d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# story_text = soup\n",
    "# story_text_clean = []\n",
    "# for p in story_text:\n",
    "#     p = p.text.strip()\n",
    "#     story_text_clean.append(p)\n",
    "# story_text_clean = [''.join(story_text_clean)]\n",
    "# story_text_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "88c71566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# story_text = soup.find_all('p')[2:]\n",
    "# story_text_clean = []\n",
    "# for p in story_text:\n",
    "#     p = p.text.strip()\n",
    "#     story_text_clean.append(p)\n",
    "# story_text_clean = [''.join(story_text_clean)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c00968",
   "metadata": {},
   "source": [
    "TO DO: Fix image scraper to get PNGs...or succesfull append something"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "12377c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# images = soup.find_all('img')\n",
    "# for image in images:\n",
    "#     if 'jpg' in image['src']:\n",
    "#         print(image['src'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e690411c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# images = soup.find_all('img')\n",
    "# png_list = []\n",
    "# for image in images:\n",
    "#     if 'png' in image['src']:\n",
    "#         png_list.append(image['src'])\n",
    "# print(png_list[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb82010",
   "metadata": {},
   "source": [
    "## Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ba246f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create Lists\n",
    "\n",
    "url_list = []\n",
    "hed_list = []\n",
    "dek_list = []\n",
    "pub_date_list = []\n",
    "lede_list = []\n",
    "story_text_list = []\n",
    "entire_soup_list = []\n",
    "\n",
    "# image_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12b991b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5851\n",
      "https://therealdeal.com/chicago/?p=263770\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "for url in df1['URL']:\n",
    "    if counter > 13000:\n",
    "        break\n",
    "    else:\n",
    "        try:\n",
    "            page = requests.get(url)\n",
    "            soup = BeautifulSoup(page.content, 'html.parser')\n",
    "            counter = counter + 1\n",
    "            clear_output(wait=True)\n",
    "            url_list.append(url)\n",
    "            print(counter)\n",
    "            print(url)\n",
    "            \n",
    "            ####################\n",
    "            # HED #\n",
    "            \n",
    "            try:\n",
    "#                 print(soup.find('h1').text.strip())\n",
    "                hed_1 = soup.find('h1').text.strip()\n",
    "                hed_list.append(hed_1)\n",
    "            except Exception as e:\n",
    "                print(f'HED fail {url} ERROR: {e}')\n",
    "                hed_list.append(f'HED fail {url} ERROR: {e}')\n",
    "            \n",
    "            ####################\n",
    "            # DEK #\n",
    "            \n",
    "            try:\n",
    "#                 print(soup.find('h2').text.strip())\n",
    "                dek_1 = soup.find('h2').text.strip()\n",
    "                dek_list.append(dek_1)\n",
    "            except Exception as e:\n",
    "                print(f'DEK fail {url} ERROR: {e}')\n",
    "                dek_list.append(f'DEK fail {url} ERROR: {e}')\n",
    "                \n",
    "            ####################  \n",
    "            # PUB_DATE #\n",
    "            \n",
    "            try:\n",
    "#                 print(soup.find('div', class_='col-7 col-md-3 order-md-3 text-right').text.strip())\n",
    "                pub_date_1 = soup.find('div', class_='col-7 col-md-3 order-md-3 text-right').text.strip()\n",
    "                pub_date_list.append(pub_date_1)\n",
    "            except Exception as e:\n",
    "                print(f'PUB_DATE fail {url} ERROR: {e}')\n",
    "                pub_date_list.append(f'PUB_DATE fail {url} ERROR: {e}')\n",
    "            \n",
    "            ####################\n",
    "            # LEDE #\n",
    "            \n",
    "            try:  \n",
    "#                 print(soup.find_all('p')[2].text.strip())\n",
    "                lede_1 = soup.find_all('p')[2].text.strip()\n",
    "                lede_list.append(lede_1)\n",
    "            except Exception as e:\n",
    "                print(f'LEDE fail {url} ERROR: {e}')\n",
    "                lede_list.append(f'LEDE fail {url} ERROR: {e}')\n",
    "                \n",
    "            ####################\n",
    "            # STORY TEXT #\n",
    "                \n",
    "            try:\n",
    "                story_text = soup\n",
    "                story_text_clean = []\n",
    "                for p in story_text:\n",
    "                    p = p.text.strip()\n",
    "                    story_text_clean.append(p)\n",
    "                story_text_clean = [''.join(story_text_clean)]\n",
    "                story_text_list.append(story_text_clean)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f'STORY TEXT fail {url} ERROR: {e}')\n",
    "                story_text_list.append(f'STORY TEXT fail {url} ERROR: {e}')\n",
    "                \n",
    "            ####################\n",
    "            # ENTIRE SOUP #\n",
    "            \n",
    "            try:\n",
    "                entire_soup_list.append(soup)\n",
    "            except Exception as e:\n",
    "                entire_soup_list.append(f'SOUP FAIL {url} ERROR: {e}')\n",
    "        \n",
    "#             print('----------')\n",
    "        except Exception as e:\n",
    "            print(f'{url} - FAIL: {e}')\n",
    "            url_list.append(f'{url} - FAIL: {e}')\n",
    "            hed_list.append(f'{url} - FAIL: {e}')\n",
    "            dek_list.append(f'{url} - FAIL: {e}')\n",
    "            pub_date_list.append(f'{url} - FAIL: {e}')\n",
    "            lede_list.append(f'{url} - FAIL: {e}')\n",
    "            story_text_list.append(f'{url} - FAIL: {e}')\n",
    "            entire_soup_list.append(f'{url} - FAIL: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78136bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(url_list))\n",
    "print('---------')\n",
    "print(len(hed_list))\n",
    "print('---------')\n",
    "print(len(dek_list))\n",
    "print('---------')\n",
    "print(len(pub_date_list))\n",
    "print('---------')\n",
    "print(len(lede_list))\n",
    "print('---------')\n",
    "print(len(story_text_list))\n",
    "print('---------')\n",
    "print(len(entire_soup_list))\n",
    "print('---------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812b076f",
   "metadata": {},
   "outputs": [],
   "source": [
    "master_list = [\n",
    "    url_list,\n",
    "    hed_list,\n",
    "    dek_list,\n",
    "    pub_date_list,\n",
    "    lede_list,\n",
    "    story_text_list,\n",
    "    entire_soup_list\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945804a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_chi = pd.DataFrame(master_list).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1df464",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_chi.columns=['URL','HED','DEK','PUB_DATE','LEDE','STORY_TEXT','SOUP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c26fb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_chi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8424b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['HED'] = hed_list\n",
    "df1['DEK'] = dek_list\n",
    "df1['PUB_DATE'] = pub_date_list\n",
    "df1['LEDE'] = lede_list\n",
    "df1['STORY_TEXT'] = story_text_list\n",
    "df1['SOUP'] = entire_soup_list\n",
    "df1['URL'] = url_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d16318",
   "metadata": {},
   "source": [
    "## Parser Playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09183b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['STORY_TEXT'] = df['STORY_TEXT'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3e6d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['URL'].iloc[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bd951d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['ADDRESS'] = df['STORY_TEXT'].str.extract(r'(?P<ADDRESS>[1-9][0-9]* (?:\\w+\\W+){1,6}(?:Road|Rd|Avenue|Ave|Boulevard|Blvd|Street|St|Place|Drive|Dr|Huron))')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4685959a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df['ADDRESS'].isna().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e210ff8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df[['URL','ADDRESS']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96ef211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.drop(columns='SOUP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e2dad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv('Chicago_sample.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aebfb49",
   "metadata": {},
   "source": [
    "## Code graveyard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b8fbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "            ####################\n",
    "            # IMAGES #\n",
    "            \n",
    "#             try:            \n",
    "#                 images = soup.find_all('img')\n",
    "#                 image_found = False\n",
    "#                 try:\n",
    "#                     while image_found == False:\n",
    "#                         png_list = []\n",
    "#                         for image in images:\n",
    "#                             if 'jpg' in image['src']:\n",
    "#                                 image_list.append(image['src'])\n",
    "#                                 print(image['src'])\n",
    "#                                 image_found = True\n",
    "#                             break\n",
    "#                         if image_found == False:\n",
    "#                             for image in images:\n",
    "#                                 if 'png' in image['src']:\n",
    "#                                     png_list.append(image['src'])\n",
    "#                                     image_found = True\n",
    "#                             image_list.append(png_list[-1])\n",
    "#                             print(png_list[-1])\n",
    "#                 except Exception as e:\n",
    "#                     print(f'No dice: {e}')\n",
    "#                 print('------')\n",
    "#             except Exception as e:\n",
    "#                 print('find_image_fail')\n",
    "            \n",
    "            ####################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
